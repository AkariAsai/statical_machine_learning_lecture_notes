\documentclass[uplatex]{jsarticle}
\usepackage[dvipdfmx]{graphicx}
\usepackage{ascmac}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{cases}

\DeclareMathOperator*{\minimize}{minimize}

\title{統計的機械学習 2017年夏学期 シケプリ}
\author{@asarihamaguri01}
\begin{document}
\maketitle

\section{はじめに}
2017年度春学期より開講の鶴岡先生による「統計的機械学習」のシケプリです。基本的には授業スライドのまとめ、\
省略されている用語の解説や勾配計算、口頭で説明された内容をまとめています。数式についてはできるだけチェックしていますが、\
もしミスなどあれば教えていただけると嬉しいです。


\section{統計的機械学習の基本}
\subsection{教師あり学習と教師なし学習(第一講)}
統計的機械学習で扱われる問題は教師あり学習」「教師なし学習」、「強化学習」に大別できます。統計的機械学習の授業では教師あり学習、教師なし学習についての扱います。

\begin{itembox}[l]{教師あり学習」と「教師なし学習}
  \begin{description}
    \item [教室き学習(supervised Learning)]\mbox{}\\
    入力から出力を予測するモデルを学習、正解が与えられている。
    \item [教師なし学習(unsupervised learning)]\mbox{}\\
    予測の対象となる出力(正解)が与えられない、データの構造や関係を解析
  \end{description}
\end{itembox}

授業で紹介されたアメリカ東海岸エリアの労働者の賃金データですWage\footnote{https://vincentarelbundock.github.io/Rdatasets/doc/ISLR/Wage.html}は\
それぞれの労働者のage, sex, year, race等の情報と賃金のデータが与えられているため、ある情報が与えられた時にその人の賃金を予測するモデルを作ること\
ができます。このように入力、出力が共に与えられ、入力から出力を予測する回帰、分類等の問題を教師あり学習と言います。

一方、遺伝子の発現データですNCI60 Data\footnote{https://www.rdocumentation.org/packages/made4/versions/1.46.0/topics/NCI60}は\
特に出力が与えられているわけではなく、クラスタリング問題であり教師なし学習に分類されます。

\subsection{記法について}
まず$p$個の特徴量を持つ$n$個入力データについては、$i$番目の観測データの$j$番目の変数を$x_{ij}$とすると、以下のようなベクトルXで表します。
\[
  X = \left(
    \begin{array}{cccc}
      x_{11} & x_{12} & \ldots & x_{1p} \\
      x_{21} & x_{22} & \ldots & x_{2p} \\
      \vdots & \vdots & \ddots & \vdots \\
      x_{n1} & x_{n2} & \ldots & x_{np}
    \end{array}
  \right)
\]
ここで$i$番目のじの観測データを表すベクトルはを$x_i^T = (x_{i1} \ x_{i2} \ \ldots x_{ip})$とすると、
\[
  \bm{X} = \left(
    \begin{array}{c}
      x_1^T \\
      x_2^T\\
      \vdots \\
      x_n^T
    \end{array}
  \right)
\]
\subsection{統計的学習とは(第3講)}
統計的学習とは、入力$X = (X_1, X_2, \ldots, X_p)$と出力$Y$の間にある関係が存在すると過程、入力データ$X$からYを推定できることを目指します。
\begin{itembox}[l]{入力と出力}
  \begin{description}
    \item[入力]\mbox{}\\
    統計的機械学習において、予測行うために与えるデータ。AdvertisementデータセットにおけるTV, Radio, newspaperなど。入力変数、予測変数、特徴量（素性）などと呼ばれる。
    \item[出力]\mbox{}\\
    学習されたデータに入力データを与えた時に返される予測結果。出力変数、応答変数、従属変数などど呼ばれる。
  \end{description}
  統計的機械学習では、以上の入力$X = (X_1, X_2, \dots , X_p)$と出力Yの間にある何らかのシステマティックな関係$f$をを予測することを目指す。
  $$Y = f(X) + \epsilon$$

\end{itembox}

またここで$\epsilon$は確率的な誤差(error)を示しており、平均はゼロとなる。例えばIncomeデータセットでは、ある人が教育を受けた年数(Years of Education)を$x$軸、収入(Income)を$y$軸でとると、\
$xy$平面状に右肩上がりの境界線を描くことができる。一方さらに年功(Seniority)を入力に加え、二つの入力変数（Years of EducationとSeniority）からIncomeを予測しようとすると、3次元空間状の曲面となる。\

このように$xとyの間に存在する関数f$を推定する目的としては、予測及び推論という二つの目的があります。
\begin{description}
  \item[目的1 : 予測(prediction)]\mbox{}\\
  Yが容易に得られない場合に、XからYを予測し、推定する。
  $$\hat{Y} = \hat{f}(X)$$
  例えば患者に初めての薬を処方する際、その患者が服用して副作用が起こる可能性があるかどうか、事前に予測する必要がる。ここで過去の副作用発生事例から\
  副作用を起こした患者、起こしていない患者の年齢、持病、性別等のデータXより、この新しい患者がある薬に対して重篤な副作用が起きるリスクいう出力$Y$を事前に推定することができる。
  このように予測が目的です時は推定結果が正確です限り、$\hat{f}$はブラックボックス化して構わない。
  \item[目的2 : 推論(inference)]
  $X_1, X_2, \dots , X_p$が変化した時にY がどのように変わるかを知りたい。これはどの入力変数がYに関係しているかを知り、活用することを目指している。\
  例えばAdvertisingデータセットにおける学習では、知りたいのは「何らかのシステマティックな関係により売り上げがいくつかになる」\
  ではなく、例えばTV広告を増やした時、新聞広告を増やした時でどのくらい売り上げが変わるのかを知るため、\
  Yと入力変数Xの間にある線形もしくは非線形な関係を知り、どのXがYに影響を与えているかそのウェイトの大きさをみる。
\end{description}
目的を予測にするか推論にするかにより、選択されるべき適切なモデルは異なってくる。\
例えば推論を目的とするならば、推定した関数の中身の解釈が比較的容易な線形モデルや、どのように条件を分岐させているかを可視化できる決定木モデルなどがどの\
独立変数がどう従属変数に影響を与えているんか容易に確認が可能です。一方線形モデルでは複雑非線形な関係に対応できず、決定機は入力データに左右されるため、\
常に高い精度が得られるわけでありません。予測のみを目的とするならば、対象が複雑な現象であっても高い予測精度が得られる可能性がある非線形モデルが適しているかもしれませんが、\
非線形モデルは中身の解釈が難しく、推論には使いにくい傾向にあります$\footnote{現在でも広く使用されている機械学習モデルの一つにSVM(サポートヴェクターマシン)というものがあり、これは\
非線形な関係にも対応でき一般的なタスクにおいて線形モデルより高い精度を発揮することも多いのですが、線形モデルと比較するとどの入力変数が出力とどの程度影響を与えているか直接知ることはできません。\
また機械学習ではないですが、今話題の深層学習もモデル自体をブラックボックス化してしまい、人間が直接的にどうそれぞれの入力変数が影響を与えているのか知る方法はありません}。$
\begin{itembox}[l]{予測精度と解釈性のトレードオフ}
  \begin{itemize}
    \item 表現力の低いモデルでは複雑な形の$f$をうまく近似できない一方、 $Y$と$X_1, X_2 \dots X_p$の関係がわかりやすい。（予測精度は低いが解釈性が高い）
    \item 表現力の高いモデルでは多様な$f$に対応が可能ですが、$Y$と$X_1, X_2 \dots X_p$の関係がわかりづらい。（予測精度は高いが解釈性が低い）
  \end{itemize}
\end{itembox}
以下のグラフは様々なグラフの予測精度と解釈性について配置したものであり、
\begin{figure}
  \begin{center}
    \includegraphics[width=13cm]{img/tradeoff.png}
    \caption{予測精度と解釈性}
  \end{center}
\end{figure}
実際に解釈性が高いモデルは予測精度が低く、解釈性の低いモデルは予測精度が高く出ることがわかります。

\section{回帰と分類}
\subsection{変数の種類}
統計的機械学習ではある一つ以上の入力変数$X$から、出力$y$を推定する$f$を求めことを目的とするが、入力変数$X$には連続した実数値となる量的変数と、\
離散的なカテゴリ（ラベル）になる質的変数がある。
\begin{itembox}[l]{量的変数と質的変数}
  \begin{description}
    \item [量的変数]\mbox{}\\
    年齢、身長、収入、株価などの連続的な実数値をとる変数。そのまま入力変数として使用することができる。
    \item [質的変数]\mbox{}\\
    性別、学歴、Yes/No、カテゴリの種類など、$K$個の異なるクラス(カテゴリ）からどれかの値をとる変数。そのままでは入力変数として用いることができず、\
    後述するダミー変数の使用などにより予測モデルに組み込む。
  \end{description}
\end{itembox}
教師あり学習は回帰問題と分類問題に大別することができ、回帰問題(regression Problem)では予測変数が金融商品の価格等の量的変数となる一方、\
分類問題(classification problem)では予測対象が債務不履行に陥ったか否かなどの質的変数になります。\\

補足ですが、質的変数を入力変数として用いた予測モデルを作る際には「女性/男性」などの情報はそのままでは数理モデルに組み込むことができないのでダミー変数をそれぞれの質的変数（質的変数に含まれるカテゴリごとに)ダミー変数を作成することが一般的です。\
例えば「女性/男性」「大人（18歳以上）/子供(18歳未満)」等ある質的変数について完全に2値で分けられるときには以下のように「女性であれば1/男性であれば2」等のように変数を作ります。\\
\[
  x^{gender}_i = \begin{cases}
    1 & ({\rm if\ ith\ person\ is\ a\ female}) \\
    0 & ({\rm if\ ith\ person\ is\ a\ male})
  \end{cases}
\]
ではこのように「true/false」の2値で分類できない場合はどうするのでしょうか。例えばethnicity(民族)について、とりあえずCaucasian = 2, African American = 1, or Asian = 0とか\
適当に数字を振ってしまったら$\footnote{どっかの団体に怒られそうな例だ...}$もともと順序や序列の存在しないデータに恣意的に大小関係を加えることになり、モデルの予測を歪めてしまいかねません。\
そこでこういった複数の値を取りうる質的変数についてはその全てに対してダミー変数を作り、「Asianならば1、そうでければ0」といった風に数字を割り当てます。
\[
  x^{asian}_i = \begin{cases}
    1 & ({\rm if\ ith\ person\ is\ an\ Asian}) \\
    0 & ({\rm if\ ith\ person\ is\ not\ an\ Asian})
  \end{cases}
\]

\subsection{モデルの精度}
モデルの精度の測り方には様々な方法があるが、回帰問題の場合、平均二乗誤差(mean squared error, MSEを用いることが一般的です。
\begin{itembox}[l]{平均二乗誤差}
    $$MSE = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{f}(x_i))^2$$
    分類問題の場合は
    $${\rm error rate} = \frac{1}{n}\sum_{i=1}^n I(y_i \neq \hat{y}_i)$$
    ここで$I(y_i \neq \hat{y}_i)$は$y_i \neq \hat{y}_i$ならば1を、そうでなければ0（すなわち正解していれば0）をとる関数です。
\end{itembox}
ちなみにこの式で表されるのは学習データでの誤差(training accuracy)ですが、本当に最小化したいのは未知データでの誤差です（test MSE）。\
$${\rm Ave}((y_0 - \hat{f}(x_0))^2)$$
$${\rm Ave}(I(y_i \neq \hat{y}_i)^2)$$
仮に学習データをかなり高い精度で学習できるモデルを構築したとしても、それがテストデータに対しても同様に高い精度で予測が可能ということはできない。
またモデルの精度を測る指標として（？）バリアンス(variance)とバイアス(bias)があります。
\begin{itembox}[l]{バリアンスとバイアス}
  \begin{description}
    \item [バリアンス]\mbox{}\\
    学習データの違いによってどれだけモデルによって予測される$\hat{f}$の値が変わるかを示す。未知のデータに対してどのくらい正しく予測できているかを測る。
    \item [バイアス]\mbox{}\\
    真のラベル$f$と予測した値$\hat{f}$がどれだけずれているかを示す。学習データに対して、どのくらい正しく予測できているかを測る。
  \end{description}
\end{itembox}
平滑化スプライン等の表現力の高いモデルは、学習モデルに対しては\
高い精度で予測できる一方、学習データにオーバーフィットしやすく新しい未知データに対して正しく予測できない可能性が高まるため、バリアンスが高い。\
一方線形回帰などの表現力のあまり高くないシンプルなモデルは、このようなオーバーフィットをしにくく、バリアンスは小さくなる傾向にあるが、学習データに対してフィットしきれずバイアスが大きくなります。

\begin{itembox}[l]{ The bias-variance trade-off}
  テストMSEは以下の3つの項の和に分解でき、バイアスとバリアンスにはトレードオフの関係にあることが確認できる。（この導出については参考資料に）
  \begin{itemize}
    \item $\hat{f}(x_0)$のバリアンス（分散）
    \item $\hat{f}(x_0)$のバイアスの二乗
    \item $\epsilon$の分散
  \end{itemize}
  $${\rm Test MSE} = E[(y_0 - \hat{f}(x_0))^2] = {\rm Var}(\hat{f}(x_0)) + [{\rm Bias (\hat{f}(x_0))}]^2 + {\rm Var}(\epsilon)$$
\end{itembox}

\subsection{ベイズ分類器}
条件付き確率が最大になるようなクラスにあるデータ$X_i$を分類する方法です。ベイズ最適決定とも呼ばれます。
\begin{itembox}[l]{ベイズ分類器}
  以下の条件つき確率が最大になるクラスを選び、期待誤り率を最小になることを目指した。
  $$P(Y = j|X = x_0)$$
  ベイズ分類器の謝り率はベイズ謝り率(Bayes error rate)と呼ばれ、以下の式で表される。\
  $$1 - E[{\rm max P_r}(Y = j|X)]$$
  これは回帰問題での削減不可能な誤差に相当している。
\end{itembox}
しかし実際のところ、条件付き確率を正しく知ることはできないので、これはあくまでも理論上の分類器になります。\
ベイズ分類器で分類された結果に基づいてデータの間に惹かれた教会のことをベイズ決定境界といいます。
ベイズ分類器とは頃なる方法で分類を行うことを目指すのが次のK最近傍法です。

\subsection{K最近傍法(K nearest meighbors method)}
K最近傍法は入力変数からそれぞれの入力が属するクラスを予測する分類問題をとくモデルの一つであり、自分に最も近い上位$K$個の観測データの属するカテゴリで多数決を行い、\
自身の最近傍$K$個の観測データの中で最も多く現れたラベルをその入力データのラベルとします。これを定式化すると以下のようになります。
\begin{itembox}[l]{k最近傍法}
  クラスの条件付き確率を以下のように推定し、この確率が最も大きくなったクラスに分類する。
    $$P_r(Y = j|X = x_0) = \frac{1}{K}\sum_{i \in N_0} I(y_i = j)$$
\end{itembox}
このKの値を小さく、例えば$K = 1$などにすると、それぞれ自身に最も近い観測データのみ考慮するため、決定境界はより入り組んだ、表現力の高いモデルになります。この場合学習データに対しては高い精度を誇るものの、\
テストデータでは謝り率が高くなることがあ理、モデルの表現力は高すぎても低すぎても、テストエラーが高くなってしまうことに注意が必要です。
KNNで求められたラベルに基づいてデータ間に惹かれた境界線がKNN決定境界になります。\
K=1の場合とK=100の場合のKNN決定境界を見ると、実際にKの数が小さいほど境界線が入り組んでいる、つまり表現力が高くなっていることがわかります。\
この例ではK=1のケースの方がわずかにK=100よりも高いテスト誤り率になってはいますが、モデルの表現力が高いからといって、テスト謝り率は必ずしも下がるわけではありません。（ただし学習謝り率についてはKの数を減らすほど小さくなう傾向にあります）\
\begin{figure}[htbp]
\begin{minipage}{0.7\hsize}
 \begin{center}
  \includegraphics[width=100mm]{img/knn_knum.png}
 \end{center}
 \caption{K=1及びK=100の時のKNN決定境界（黒太線）}
 \label{fig:one}
\end{minipage}
\begin{minipage}{0.3\hsize}
 \begin{center}
  \includegraphics[width=50mm]{img/knn_eror.png}
 \end{center}
 \caption{Kの数に対する学習謝り率とテスト謝り率の推移}
 \label{fig:two}
\end{minipage}
\end{figure}

\subsection{線形回帰(Linear regression)}
\subsubsection{線形単回帰}
線形単回帰とは、$XとY$の関係は線形ですと仮定して、一つの予測変数$X$によって量的応答変数$Y$を予測する。
\begin{itembox}[l]{線形単回帰}
  XとYの関係は以下の式のような線形で表されると仮定する。
    $$Y \approx \beta_0 + \beta_1X$$
    学友データによって適切なパラメータを予測し、$\hat{y} = \hat{\beta_0} + \hat{\beta_1}x$をより元の学習データの結果と近づけることを目指す。
    学習データを${(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)}$とし、予測のずれを残差$e_i = y_i - \hat{y_i}$とすると、線形単回帰の目的関数は以下のように表される。
    {\rm RSS}(残差平方和)
    $${\rm RSS} = e_1^2 + e_2 ^2 + e_3^2 + \ldots e_n^2 = (y_1 - \hat{\beta_0} - \hat{\beta_1}x_1)^2 +  (y_2 - \hat{\beta_0} - \hat{\beta_1}x_2)^2 \ldots$$
\end{itembox}
ここで{\rm RSS}は学習データ$X$に対して推定されたパラメータ$\hat{\beta_0}及び\hat{\beta_1}$を用いて線形のモデルで予測した$\hat{y}$の値が、実際にそれぞれの$y$とどのくらいずれているかを二乗したものの和であり、\
これを最小化することによりよりずれの少ないモデルを構築することができます。すなわち、線形単回帰においてパラメータを推定するときは残差平方和が最小になるようにパラメータを推定すればいいことになります。\
目的関数${\rm RSS}$を最小化する$\hat{\beta_0}及び\hat{\beta_1}$は、{\rm RSS}を$\hat{\beta_0}及び\hat{\beta_1}$で微分した時に、それが0なるような$\hat{\beta_0}及び\hat{\beta_1}$を求めれば良いため
$$\frac{\partial {\rm RSS}}{\partial \hat{\beta_0}} = 0, \frac{\partial {\rm RSS}}{\partial \hat{\beta_1}} = 0$$を満たす$\hat{\beta_0}及び\hat{\beta_1}$を求めると、以下のようになります。（詳細な変形などは参考資料に）
$$\hat{\beta_1} = \frac{\sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y})}{\sum_{i=1}^n (x_i - \overline{x})^2}, \hat{beta_0} = \overline{y} - \hat{\beta_1}\overline{x}$$
ここで$\overline{x} = \frac{1}{n}\sum_{i-1}^n x_i, \overline{y} = \frac{1}{n}\sum_{i-1}^n y_i$です。
この結果求められた二つのパラメータ$\hat{\beta_0}, \hat{\beta_1}$がどの程度正確なのかは母回帰直線を推定することにより求めることができます。

\subsubsection{残差{\rm RSS}に関する性質}
また、{\rm RSS}を最小にする$\beta_0, \beta_1$の導出より、以下の二つの性質が求められます。(導出は参考資料に)
\begin{equation}
  \sum_{i=1}^n e_i = 0
\end{equation}
\begin{equation}
  \sum_{i=1}^n e_ix_i = 0
\end{equation}
式(1)は残差の合計が0ですことを、式(2)は残差$e_i$と入力$x$はベクトルとして直行ですことを意味しています$\footnote{ベクトル a,b の内積がゼロですとき二つのベクトルは直行しているため。}$。

\subsubsection{線形単回帰の信頼性}
線形単回帰においては、母集団において$X, Y$の間に線形の関係性があると仮定しデータよりそのパラメータ$\beta_0, \beta_1$を予測しており本来的に母集団の$\beta_0, \beta_1$を予測することはできません。\
そこでデータより得られたパラメータがその程度信頼できるのかを確認します。\
母集団でのXとYの関係を$Y = \beta_0 + \beta_1X + \epsilon$とすると、$\epsilon$は母集団における統計的な誤差なので、母集団での回帰直線は$Y=\beta_0 + \beta_1X$であらわsれる。\
以下のように、$Y = 2 + 3X + \epsilon$という線形の関係を元に生成した人虎データに対する線形単回帰モデルに基づくパラメータ推定の結果は以下のようなグラフとなり、実際の母回帰直線と最小二乗による回帰直線の結果にややずれがあることがわかる。\
また与えられた入力データにおける誤差項により異なる観測データに対する最小二乗回帰直線にはややズレがあることが右図よりわかります。
\begin{figure}
  \begin{center}
    \includegraphics[width=13cm]{img/lienar_error.png}
    \caption{線形単回帰誤差}
  \end{center}
\end{figure}
では求められたパラメータより基礎統計や数理手法でやったように信頼区間を求めるにはどうしたらいいのでしょうか。そのためにまず各パラメータの標準誤差を求める必要があります。\
個々の観測データの誤差は独立で分散が$\sigma ^2$と仮定するとパラメータの標準誤差は
$$SE(\hat{\beta}_0^2) = \sigma^2 \left[\frac{1}{n} + \frac{\overline{x}^2}{\sum_{i=1}^n (x_i - \overline{x})^2}\right]$$\
$$SE(\hat{\beta}_1 ^2) = \frac{\sigma^2}{\sum_{i=1}^n (x_i - \overline{x})^2}$$
ここで$\sigma$は未知ですため、{\rm RSS}(残渣平方和)を用いて以下のように観測データから推定されRSE(redidual standard error)と呼ばれます。
$${\rm RSE} = \sigma \approx \sqrt{{\rm RSS}/(n-2)}$$
SEの導出、及び次の信頼区間の導出については参考資料にあります。
パラメータの信頼区間について、95$\%$の確率でその区間にパラメータの真の値が含まれるための近似的な信頼区間はそれぞれ
$$\hat{\beta}_0 \pm 2{\rm SE}(\hat{\beta}_0), \hat{\beta}_1 \pm 2{\rm SE}(\hat{\beta}_1)$$

\subsubsection{線形単回帰モデルがどの程度データにフィットしているか評価する}
モデルがどの程度データにフィットしているか評価する指標として、RSD及び決定係数$R^2$があります。
決定係数とは回帰分析によって求められた目的変数の予測値が実際の目的変数の値とどのくらい一致しているかを表している指標です。厳密な定義としては「回帰分析をした結果が目的変数のばらつき分散をどれくらい説明しているか」ということを表しており、\
もちろんですが1に近いほどデータを正しく説明できていることになります。
\begin{itembox}[l]{RSE(residualstandarderror)}
  誤差$\epsilon$の標準偏差の推定量のこと。以下の式で求めることができる。
  $$RSE = \sqrt{\frac{1}{n-2}{\rm RSS}} = \sqrt{\frac{1}{n-2}\sum_{i=1}^n(y_i - \hat{y}_i)^2}$$
\end{itembox}
次に紹介する決定係数も回帰モデルによって実データをどれくらい説明できているか、つまり回帰分析の制度を表す指標です。\
こちらも1に近いほどモデルがデータをよく表現できていることを意味します。
\begin{itembox}[l]{決定係数$R^2$}
  決定係数とは応答変数$Y$の分散を$X$による回帰方程式で減らした割合。以下の式で定義される。
  $$R^2 = \frac{TSS - {\rm RSS}}{TSS} = 1 - \frac{{\rm RSS}}{TSS}$$
  ここでTSS(Total sum of square)とはもともとのデータが持っている分散の総和であり、以下の式で定義される。
  $$TSS = \sum_{i=1}^n (y_i - \overline{y})^2$$
\end{itembox}
式からわかるように、第2項が小さいすなわち${\rm RSS} \ll TSS$となり、元々のデータの持つ変動よりも回帰により予測された値と元データの間の変動の方が小さく、\
モデルがうまくデータを説明できていることを意味します。

\subsubsection{多重線形回帰}
実際のデータにおいては、一つの入力変数だけでなく、複数の変数を用いて予測をしたいというシチュエーションも多く存在します。複数の単回帰モデルを個別に構築し、組み合わせるだけでは\
予測変数同士の関係を考慮することができず、またどう個別のモデルを組み合わせるべきかは明らかになっていません。そこで多重線形回帰は、複数の予測変数を使う単一のモデルで回帰を行うことを目指しています。
\begin{itembox}[l]{多重線形回帰}
  以下のように複数のXとYの関係を以下のようなモデルで回帰する。
    $$Y \approx \beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_pX_p$$
    $\beta_j$は他 予測変数を固定して、$X_j$を1単位増やしたときに増えるY量の量を表している。
    学習データによって適切なパラメータを予測し、$\hat{y} = \hat{\beta_0} + \hat{\beta_1}x_1 + \hat{\beta_2}x_2 + \dots + \hat{\beta_p}x_p$を予測とし、学習データの残差平方和を以下のように表す。
    {\rm RSS}(残差平方和)
    $${\rm RSS} = \sum_{i=1}^n (y_1 - \hat{\beta_0} - \hat{\beta_1}x_{i1} - \hat{\beta_2}x_{i2} \ldots - \hat{\beta_p}x_{ip})^2 $$
\end{itembox}
要は線形単回帰です一つの変数について、その入力変数が一単位増えたときに増加するYの量(パラメータ$\beta_1$)がモデルの傾きになっていましたが、\
多重線形回帰ではあるp個の入力変数$X_1, X_2, \ldots X_p$それぞれにパラメータ$\beta_i$を求めてモデルを構築しただけです。
多重線形回帰についても単線系回帰の場合と同様に、それぞれのパラメータで目的関数を微分したものが0となるよう計算することで求めることができます。\
勾配情報を使って再急降下法、 準ニュートン法などを行うことによって計算できます。また、多重線形回帰の結果求められるのは回帰「平面」になります。(次元増えるからそれはそう)

\subsubsection{どの予測変数が重要なのか}
多重線形回帰を行う際に、全ての変数を元にYを求めるても逆にノイズが混じってしまったり計算のコストがかかってしまったりと有効ではありません。\
また全ての予測変数の考えうる組み合わせ全てを試して精度を調べるのもコストが大きいです。そこで多重線形回帰では事前にどの予測変数が重要ですかの見当をつけモデルを構築するのが一般的です$\footnote{Azu○eML勉強会で関係なさそう、もしくはidなどノイズを与えてそうなHousingデータの\
入力変数を除いて見たりした人もいたのではないでしょうか。要はあんな感じです。}$。変数選択方には以下のような手法が一般的です。
\begin{itembox}[l]{変数選択法の種類}
\begin{description}
  \item[変数増加法]\mbox{}\\
  予測変数を全く入れない状態からモデルの構築を始め、ある一つの変数だけを今あるモデルに加えたとき、その{\rm RSS}が最も小さい（つまりある変数を加えることによりよりデータを説明できるモデルを作ることのできる）予測変数を追加するということを繰り返す。
  \item[変数減少法]\mbox{}\\
   全ての予測変数を使うモデルから出発し、p値が最も大きい（つまり結果と関係の小さい）予測変数を削除するということを繰り返す
  \item[変数増減法]\mbox{}\\
  変数増加法と変数削減法を組み合わせたもの。
\end{description}
\end{itembox}
蛇足かもしれませんが、p値とは帰無仮説$H_0$のもとで，統計量（確率変数）Tが、データから実際に計算した統計量の値$T_0$よりも「極端」な値を取る確率を意味し、\
実用的な意味でいうとp値が有意水準（5$\%$）とかより小さければ帰無仮説を棄却します。

\subsubsection{多重線形回帰モデルがどの程度データにフィットしているか評価する}
線形単回帰モデルと同様に、多重線形回帰においてモデルがどのくらいデータに適合しているかは決定係数及びRSEによって評価することができます。
決定係数は線形単回帰と同様、$$R^2 = 1 - \frac{{\rm {\rm RSS}}}{{\rm TSS}}$$で表すことができますが、多重線形回帰の場合は1+pだけ自由度が減少するため、以下の式で定義されます。
$${\rm RSE} = \sqrt{\frac{1}{n-1 -p}{\rm {\rm RSS}}}$$

\subsubsection{多重線形回帰モデルにおける変数同士の相互作用}
線形単回帰モデルの時は入力変数は一つだけだったため、単一の入力変数と予測変数の間の関係のみに着目するだけですみました。\
しかし多重線形回帰については、描く予測変数間の応答変数に対する効果は必ずしも独立しているとは言えず（例えばAdvertisementデータにおける\
TVとradioという二つの変数はそれぞれ独立して売り上げに貢献しているのではなく、その二つを一緒に行うことによってより宣伝効果をあげているかもしれません）、多数の変数間の相互作用を考える必要があります。\
それぞれの変数が独立だとした時のモデルは以下のように関連していると考えられる変数の積を新たな変数としてモデルに追加することで実現できます。
$$Y = \beta_0 + \beta_1 X_1 + \beta_3 X_1X_2$$

\subsubsection{多重線形回帰で起こりうる問題}
多重線形回帰では線形回帰モデルで解決できない非線形な関係に対応することを目指していました。しかし多重線形モデルでは以下のような問題も発生してしまいます。
\begin{description}
  \item[誤差項の相関]\mbox{}\\
  時系列データ(time-series data)などでは誤差項に 相関があることが多い。(so what)
  \item[誤差項の相関]\mbox{}\\
  $Y$の値が大きいところでは誤差項の分散も大きくなってしまう。そこでYを予測するのではなく、$\log{Y}$を予測するようにするとこれを抑えることができる。
  \item[外れ値]\mbox{}\\
  計測エラーなどによる外れ値が推論に大きな影響を与える可能性があり、こういった外れ値は回帰直線には影響を与えにくいが決定係数などに大きな影響を与えうる。そこで絶対値が3を超えるような値は外れ値の可能性が高いため、\
  除外をした方が良い。
  \item[てこ比]\mbox{}\\
  \item[共線性(collinearity)]
  予測変数の間に強い関連がある場合間に強い相関がある場合、そのままではパラメータ推定の信頼性が落ちてしまう。\
  そのため相関行列により2つの予測変数の間の関係を検出する必要があり、仮に二つの予測変数が強い相関を持っていた場合、これらを一つに統合する。
  \item[多重共線性]\mbox{}\\
  三つ以上の変数の間に共線性がある場合、VIFによって検出が可能である。
  $$VIF(\hat{beta}_j) = \frac{1}{1 - R^2_{X_j|X_{-j}}}$$
\end{description}

\subsubsection{多項式回帰(polynomial regression)}
これまでは全ての変数について、ある入力変数$X^p$の予測変数$Y$にに与える影響は線形的であるとしてモデルを考えてきました。\
しかし実際にはYoutubeで流れる広告と売り上げの間に二次関数的な関係性が存在する可能性もあります。こういった非線形な関係を考慮するためには多項式回帰を行います。\
予測変数を追加する必要があります。

\subsection{パラメトリックモデルとノンパラメトリックモデルの比較}
線形単回帰や多重単回帰のようにデータから目的関数を最小化するようなパラメータを推定し、そのパラメータに基づいてある入力$X$に対して予測を行うモデルをパラメトリック、\
KNNのようにパラメータを推定するのではなく逐次元のデータを読み込み、予測するモデルをノンパラメトリックモデルと言います$\footnote{パラメトリックモデルとノンパラメトリックモデル http://www.snap-tck.com/room04/c01/stat/stat99/stat0103.pdf}$。
与えられたデータに非線形性が強いなど、ノンパラメトリックな手法の方が有利に働くケースも存在しないわけではないのですが、\
パラメトリックなモデルノンパラメトリックなモデルの精度が同じくらいなのではあればパラメトリックなモデルの方が良いとされることが多いです。
理由としては
\begin{itemize}
  \item ノンパラメトリック手法では一度学習したデータを保持し続ける必要があり、一度学習すればあとはパラメータ飲み保持して予測を行うことができるパラメトリックなモデルより取り回しのしにくいという点があります。\
  \item ノンパラメトリックなモデルの方が次元の呪い（入力変数の数が増える、すなわち予測する空間の次元が大きくなるほど適切な予測ができにくくなる）の影響を強く受けてしまう
\end{itemize}
ことが挙げられます。


\section{分類}
\subsection{分類問題とは}
分類問題とは応答変数が質的変数です予測問題であり、クラスを予測する問題とも言えます。\
例えば患者の症状から疾患を予測する、メールがスパムかどうかを判定するなどはこのような分類問題の例です。\
ところでなぜ分類問題と回帰問題を別の問題として考えなくてはいけないのでしょうか。これは特に3クラス以上の分類などにおいて、\
線形回帰により分類を行おうと考えると、まずカテゴリに順序等をつけ出てくるスコアの大きさに応じて三段階に振り分ける、などそもそも明確にカテゴリ同士に順序がないはずのものについても\
恣意的な仮定を置く必要が出てきてしまい、正しい予測をできない可能性があるためです。

\subsection{ロジスティック回帰による分類問題}
以下では回d帰の中でも一般的な、ロジスティックモデルについて説明します。
\begin{itembox}[l]{ロジスティックモデル}
  ロジスティックモデルとは、以下のロジスティック関数を用いて確率値を計算するモデル。
  $$p(x) = \frac{e^{\beta_0 + \beta_1x}}{1 + e^{\beta_0 + \beta_1x}} = \frac{1}{1 + e^{-(\beta_0 + \beta_1x)}}$$
  \begin{description}
    \item[オッズ]\mbox{}\\
    $$\frac{p(x)}{1 - p(x)} = \exp{\beta_0 + \beta_1 x}$$
    \item[ロジット]\mbox{}\\
    $$\log{\frac{p(x)}{1 - p(x)}} = \log{\exp{\beta_0 + \beta_1 x}} = \beta_0 + \beta_1 x$$
  \end{description}
\end{itembox}

ロジスティック回帰におけるパラメータは確率モデルのパラメータ推定に広く使われる、最尤推定により決定されます。
\begin{itembox}[l]{最尤推定}
  パラメータの尤度(学習データが得られる確率)を計算する以下のような関数を尤度関数といい、これを最大にするような$\beta_0, \beta_1$の値を計算する。
  $$l(\beta_0, \beta_1) = \prod_{i:y_i = 1}p(x_i)\prod_{i':y_i' = 0}(1 - p(x_i'))$$
\end{itembox}
ロジスティック回帰で求められたパラメータの値から予測値を求める方法は、以下のようにある人の講座残から債務不履行(default)する確率を予測するモデルのパラメータをロジスティック推定により求めた結果が以下の場合、
\begin{figure}
  \begin{center}
    \includegraphics[width=13cm]{img/logit.png}
    \caption{ロジスティック回帰により求められた値}
  \end{center}
\end{figure}
切片($\hat{\beta}_0$)が-10.6513、balanceについて1単位増加させた時の予測変数の増加分$\hat{\beta}_1$が0.0055となるので、例えば$\$ 1000$の人がdefaultする確率は
$$\hat{p}(X) = \frac{\exp{(\hat{\beta}_0 + \hat{\beta}_1 X)}}{1 + \exp{(\hat{\beta}_0 + \hat{\beta}_1 X)}} = \frac{\exp{(-10.6513 + 0.0055 \times 1000)}}{1 + \exp{(-10.6513 + 0.0055 \times 1000)}} = 0.00576$$
よってbalanceが$\$$1000の人がデフォルトする確率は0.5$\%$と極めて低いことがわかる。
質的変数（あるカテゴリに対してそれぞれダミー変数を設定したもの）を用いて予想を行う場合は以下のような学生であるかどうかというダミー変数に対して予測されたパラメータを用いて予測を行う場合、学生である人は$x = 1$、学生でない人は$x = 0$とすれば良いので、学生のデフォルトする確率は
\begin{figure}
  \begin{center}
    \includegraphics[width=13cm]{img/logit_quali.png}
    \caption{ロジスティック回帰により求められた値(定性情報)}
  \end{center}
\end{figure}
$$\hat{p}(X) = \frac{\exp{(\hat{\beta}_0 + \hat{\beta}_1 X)}}{1 + \exp{(\hat{\beta}_0 + \hat{\beta}_1 X)}} = \frac{\exp{(-3.5041 + 0.4049 \times 1)}}{1 + \exp{(-3.5041 + 0.4049 \times 1)}} = 0.00431$$
上記の式でx=0とした時の確率が0.0292より、学生の方がややデフォルトする確率が高いとこのモデルからは予測できます$\footnote{実際には後述するように必ずしも学生がデフォルトしやすいわけではなく交絡によりそう見えているだけだったりします。}$。

またロジスティック回帰において応答変数（予測するラベル、defaultするかどうか、患者の病気の症状など）が多数の値を取る場合は、ソフトマックス回帰により、最も確率が高くなるラベルを選択肢します。
\begin{itembox}[l]{ソフトマックス回帰}
  ソフトマックス関数は、$n$次元の実数ベクトル $ x = (x_1 \dots  x_n)$を受け取って以下で定義される$n$次元実数ベクトル$y = (y_1 \dots y_n)$を返す関数である。
  $$y_i = \frac{e^{x_i}}{e^{x_1} + e^{x_2} \ldots + e^{x_n}}$$
  ここである予測変数のカテゴリkについて、$e_k = \exp{(\sum_p \beta_{kp}X_p)}$とすると、
  $${\rm softmax}(x)_k = P(Y = k|X) = \frac{e_k}{\sum_{k=1}^n e_i}$$
\end{itembox}
ソフトマックス回帰で行なっているのは、ラベルの数が$n$個存在していたとき、その$k$番目のラベル（カテゴリ)の発生確率はそれぞれのオッズを正規化（オッズを足し合わせて、$k$番目のラベルのオッズをその総和で割っている）しある入力$X$が与えられた時、それが多数うカテゴリの中の$k$番目のラベルである確率\
を求めています。つまり、それぞれのカテゴリの予測確率を足し合わせると必ず1になります$\footnote{ちなみにみんな大好きDEEP LEARNINGではとてもよくでてきます。きになる人は「mnist sotmax」とかでググってみてください。}$。

\subsection{複数の予測変数を用いたロジスティック回帰}
線形単回帰と同様に、ロジスティック回帰も複数の予測変数を取り入れたモデルに拡張することができます。複数の予測変数を利用したロジスティック回帰を多重ロジスティック回帰といいます。
\begin{itembox}[l]{多重ロジスティック回帰(multiple logistic regression)}
  多数の予測変数を用いたロジスティック回帰モデルを多重ロジスティック回帰という。
  $$ロジット = \frac{p(X)}{1 - p(x)} = \beta_0 + \beta_1 X_1 + \beta_2 X_2 \ldots \beta_p X_p$$
  $$p(X) = \frac{\exp{(\beta_0 + \beta_1 X_1 + \beta_2 X_2 \ldots \beta_p X_p)}}{1 + \exp{(\beta_0 + \beta_1 X_1 + \beta_2 X_2 \ldots \beta_p X_p)}}$$
\end{itembox}
多数ロジスティック回帰についても、計算方法は入力変数を一つにしたロジスティック回帰と基本的には同じです。先ほどと同様に、balance, income, studentを使ってある人がdefaultするかどうかを予測してみます。
\begin{figure}
  \begin{center}
    \includegraphics[width=13cm]{img/log_mul.png}
    \caption{多重ロジスティック回帰}
  \end{center}
\end{figure}
balanceが$\$$1,500、incomeが゙$\$$40,000の学生がdefaultする確率は、上の表で切片$ \beta_0 = −10.8690$、balanceにかかるパラメータ$\beta_1 = 0.0057$、incomeにかかるパラメータ$\beta_2 = 0.0030$、studentにかかるパラメータ$\beta_3 = −0.6468$より、\
$$\hat{p}(X) = \frac{\exp{(−10.8690 + 0.0057 \times 1500 + 0.0030 \times 40,000 + (−0.6468) \times 1)}}{\exp{(−10.8690 + 0.0057 \times 1500 + 0.0030 \times 40,000 + (−0.6468) \times 1)}} = 0.058$$
ちなみに式からわかるかもしれませんが、このモデルにおいて学生かどうかを表すダミー変数にかかる$\beta_3 < 0$となっているため、\
同じbalance, incomeであれば\.学\.生\.で\.は\.な\.い方がdefaultする確率は低いと予測されます。これが先ほど軽く補足した交絡(confounding)による効果であり、\
交絡とは統計モデルの中の従属変数と独立変数の両方に（肯定的または否定的に）相関する外部変数が存在することを意味します。例えば「飲酒者と非飲酒者では飲酒者の肺癌発生率が高くなる」というのがデータからは確かに観測されるのですが、\
これは直接的に飲酒が肺がんの要因になっているわけではなく$\footnote{もしかしたら何か関係あるのかもしれないけどそういうのは医学部の人に聞いてください}$、非飲酒者と比較して飲酒者における喫煙者の割合が高いために、\
結果的にデータから「飲酒者の方が肺がんになりやすい」という傾向がでてしまうことになります。今回の問題では実際には学生だからdefaultを起こしやすいのではなく、学生の方が非学と比べincomeが少ない、credit card balanceが大きい等の要因によりデフォルト率が高くなっています。

\subsection{線形判別分析}
ロジスティック回帰のように直接${\rm Pr}(Y| X = x)$ をモデル化するのではなくある入力変数$X$が与えられた時にそれがある応答変数$Y$となる確率を予測する別のモデルに「線形判別分析(LDA)」があります。\
\begin{itembox}[l]{線形判別分析(linear discriminant analysis)}
  線形判別分析とは、${\rm Pr}(Y| X = x)$を正規分布でモデル化し、ベイズの定理によって${\rm Pr}(Y| X = x)$を計算するモデル。
  $K$をクラスの数、$\pi_k$をラス$k$の事前分布、$f_k(x)$をクラス$k$のデータの密度関数とすると、${\rm Pr}(Y| X = x)$は以下のように表される。
  \begin{equation}
    \begin{split}
      {\rm Pr}(Y| X = x) &= \frac{{\rm Pr}(Y = k){\rm Pr}(X = x| Y = k)}{{\rm Pr}(X = x)} \\
      &\frac{{\rm Pr}(Y = k){\rm Pr}(X = x| Y = k)}{\sum_{l=1}^K{\rm Pr}(X = x, Y = l)}\\
      &=\frac{{\rm Pr}(Y = k){\rm Pr}(X = x| Y = k)}{\sum_{l=1}^K{\rm Pr}(Y = l){\rm Pr}(X = x|Y = l)}\\
    \end{split}
  \end{equation}
    \begin{equation}
      p_k(x) = \frac{\pi_kf_k(x)}{\sum_{l=1}^K \pi_lf_l(x)}
    \end{equation}
\end{itembox}
$f_k(x)$が正規分布だと仮定すると、$f_k(x)$は以下の式で表される。
\begin{equation}
  f_k(x) = \frac{1}{\sqrt{2\pi}\sigma_k}\exp{\left(- \frac{1}{2\sigma_k^2}(x - \mu_{x_k})^2\right)}
\end{equation}
分散が全てのクラスで等しい場合、ある入力$x$が与えられた時、それが$k$である確率は式(5)を式(4)に代入することで求めることができ、\
\begin{equation}
        p_k(x) = \frac{\pi_k \frac{1}{\sqrt{2\pi}\sigma_k}\exp{\left(- \frac{1}{2\sigma_k^2}(x - \mu_{x_k})^2\right)}}{\sum_{l=1}^K \pi_l\frac{1}{\sqrt{2\pi}\sigma_l}\exp{\left(- \frac{1}{2\sigma_l^2}(x - \mu_{x_l})^2\right)}}
\end{equation}
式(6)より、$p_k(x)$は$\pi_k \frac{1}{\sqrt{2\pi}\sigma_k}\exp{\left(- \frac{1}{2\sigma_k^2}(x - \mu_{x_k})^2\right)}$に比例するため、\
これが最も大きくなるクラスに入力データを分類すればいいことがわかります。
\section{リサンプリング法}
リサンプリング法とは、手持ちの限られたデータをある固定された割合で分割し、訓練データとテストデータに分けて学習するのではなく、\
ランダムサンプリングと学習によるパラメータ推定を複数回行い、データの分割の仕方による予測モデルのばらつきなどを少なくすることを目指しています。\
試行回数が増える分計算コスト画像化するため、かつてはこれがネックになっていましたが、計算機の能力向上によりこういったコストの問題は小さくなっています。\
以下で代表的な理サンプリング手法について紹介します。
\subsection{検証セットによる手法}
データセットをランダムに「学習用データセット(training set)」と「検証用データセット(validation set)」の二つにランダムに分割し、\
学習用データでモデルのパラメータを推定し、検証用データで精度を評価するという、最もベーシックな方法です。シンプルな一方で、\
データをどう分割するかによって制度の推定値やパラメータの予測値は大きく変わってしまうこと、\
またデータ自体が少ない時にデータ全体の数割から半分ほどのデータを学習に用いることができないため、制度も本来達成可能な制度より低めに推定される可能性があります。

\subsection{交差検証}
検証セットによる精度を測定する方法を補うため、交差検証法はデータセットの全てのデータを用い、また繰り返し学習したパラメータの平均値を取ることでデータの分割の仕方による\
ばらつきの影響を排除することを目指しています。
\begin{itembox}[l]{Leave-One-Out Cross Valdation(LOO黄砂検証)}
  N個の学習データ${(x_1, y_1), (x_2, y_2) \ldots (x_n, y_n)}$のうち、1つをのぞいたN-1個で学習を行い、毎回残された1個のデータで性能を評価、\
  これを全ての事例に対して行いその平均値をとsることで、最終的な評価指標の推定値とする。\\
  $$ {\rm MSE_1} = (\hat{y}_1-y_1)^2$$
  $$\dots$$
  $$ {\rm MSE_n} = (\hat{y}_n-y_n)^2$$
  $$ {\rm CV(n)} = \frac{1}{n}\sum_{i=1}^n MSE_i$$
\end{itembox}
LOO交差検証には以下のようなメリットデメリットが存在します。
  \begin{description}
    \item [LOO交差検証のメリット]\mbox{}\\
    学習には毎回n-1個のデータが使用されn個全てのデータを用いた際とほぼ同じ性能のモデルを作ることができる。\
    また分割方法の違いによる性能評価のばらつきもない。
    \item [LOO交差検証のデメリット]\mbox{}\\
    n回の学習が必要になり、かつ学習データと検証データに一定割合で分割する際と比べ学習に\
    使用するデータの数も増えるため、データの数が多いほど、学習に必要なコストが大きく増加する。
  \end{description}

このように、LOO交差検証は学習データと検証データに分割する際のデメリットを克服できる一方、\
計算コストが大きく増加してしまいます。そこで、一つだけを性能評価用に残し、n回学習を行うのではなく、\
データをある数$k$に分割し、一つのグループ以外のデータで学習、残された一つのデータで性能検証を行うことを繰り返すことで\
計算量を抑えつつも性能の向上を目指すのが次のk分割交差検証です。
\begin{itembox}[l]{k-fold Cross Valdation(k分割交差検証)}
  データを$k$個のグループに分割し一つを検証データ、残りの$k+1$個を学習データとしてMSEを計算し、これをk回繰り返して平均を取り、\
  この平均をテストMSEとして推定する。一般的に$k=5, 10$等が用いられる。
  $$CV(k) = \frac{1}{k}\sum_{i=1}^k {\rm MSE}_i$$
\end{itembox}
LOO交差検証では一般にn回の学習を行う必要があったが、$k$分割時黄砂検証では学習回数はたかだか$k$回ですむ一方、LOO交差検証と同様、\
$k$回の学習を通して全ての入力データをみることができます。またLOO及び$k$分割交差検証の間には以下のようなBias-variance trade-offが存在します。
\begin{itembox}[l]{Bias-variance trade-off}
  \begin{itemize}
    \item $k$分割交差検証の場合、学習に使えるデータの数はLOO比べ少なくなるので、バイアスが大きくなる可能性がある。
    \item LOO交差検証には学習に使うデータが毎回ほとんど同じなので出力間に強い相関があり、バイアスが大きい。
  \end{itemize}

\end{itembox}
\subsection{ブートストラップ}
また、こういったデータを分割し、繰り返し学習左折以外に、ブートストラップという方法もあります。
 \begin{itembox}[l]{ブートストラップ}
   標本からの復元抽出によるランダムサンプリングを繰り返すことで行う統計的推論手法。確率モデルのパラメータの信頼区間などを簡単に求めることができる。
 \end{itembox}
 復元抽出とは、抽出を行う際に一度抽出したサンプルが再び抽出の対象となりうる方法のこと。例えば受験数学でよくでてくる「赤い玉3個、白い玉4個入った袋から中身を見ずに一個取り出し色を確認して戻す作業を3回行った時、\
 全て赤い玉であった確率は」などは復元抽出の例であり、「一度出した玉は元に戻さない」であればこれは非復元抽出になります。具体的なブートストラップ方の利用としては、
 \begin{enumerate}
   \item 手持ちのデータセットからランダムサンプリングによって新たなデータセットを生成。
   \item この抽出されたデータを元にモデルのパラメータを推定する。
   \item これを多数回繰り返す。
   \item 得られたパラメータの分布より信頼区間などを計算する。
 \end{enumerate}

\section{正則化（第10講）}
\subsection{パラメータ推定法と縮小推定法}
最小二乗法、最尤推定法などのパラメータ推定方では、モデルがデータにフィットするようにパラメータを最適化する。\
一方リッジ回帰、Lasso回帰は縮小推定に分類され、各パラメータの絶対値が大きくならないようにする。正則化ともいう。\
最小二乗法による線形回帰パラメータ推定は、以下の{\rm RSS}を最小化にするパラメーラ$\beta_0, beta_1..., \beta_p$を決定します。\
これは学習データへのフィティングしか考慮していないため、過学習してしまいやすく、未知のデータに対して適切に予測できない恐れがあります。\
そこでリッジ回帰やロッソ回帰などの縮小推定法により過学習を回避し、未知データに対しても頑健なモデルを構築します。

\subsection{リッジ回帰}
\begin{itembox}[l]{リッジ回帰}
  リッジ回帰の目的関数は以下で表される。
  $$\sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p \beta_jx_{ij}\right)^2 + \underline{\lambda \sum_{j=1}^p \beta_j^2}$$
下線で示され項がパラメータの絶対値が大きくなることに対するペナルティ（L2正則化項）となる。\\
$\lambda$ : チューニングパラメータ。
\end{itembox}
L2正則化項の$\lambda$を大きくするほど、各パラメータの値$\beta_0, \beta_1..., \beta_p$はゼロに近づく。\
またリッジ回帰においては、値の大きな予測変数が大きな影響力を持つようになるため予測変数のスケールが性能に大きな影響を与えてしまいます。\
そのため、前処理として以下のような式で予測変数の標準化を行う必要があります。
\begin{equation}
  \widetilde{x_{ij}} = \frac{x_ij}{\sqrt{\frac{1}{n} \sum_{i=1}^n (x_ij - \overline{x_j})^2}}
\end{equation}

$\lambda$を大きくするとバリアンスが小さくなる。しかしあまりにも小さくすると、$\beta_1, \beta_2..., \beta_p$のパラメータはに0近づき、\
元のデータから離れすぎてしまうため、今度はバイアスが大きくなってしまいます。そこでちょうどいい$\lambda$を選択することにより、バイアス、バリアンスを\
小さくすることができます。
\begin{equation}
  E[(y_0 - \hat{f}(x_0))^2] = {\rm Var}(\hat{f}(x_0)) + [{\rm Bias}\hat{f}(x_0)]^2 + {\rm Var}(\epsilon)
\end{equation}
\begin{figure}
  \begin{center}
    \includegraphics[width=13cm]{img/lidge_lambda.png}
    \caption{Creditデータセットに対してリッジ回帰}
  \end{center}
\end{figure}
以下のグラフのバツ印のところで、バイアス、バリアンス共に小さくなっており、予測において使用すると良い$\lambda$の値になっていることがわかります。

\subsection{Lasso回帰}
Lasso回帰は、正則化項をパラメータの二乗ではなく、絶対値とした目的関数を最小化することによりパラメータを求める方法です。
\begin{itembox}[l]{Lasso回帰}
  Lasso回帰の目的関数は以下で表される。
  $$\sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p \beta_jx_{ij}\right)^2 + \underline{\lambda \sum_{j=1}^p |\beta_j|}$$
下線で示され項がパラメータの絶対値が大きくなることに対するペナルティ（L1正則化項）となる。\\
$\lambda$ : チューニングパラメータ。
\end{itembox}
Lasso回帰の目的関数はリッジ回帰と比べ、最後の正則化項が異なっているだけに見えますが、この結果得られるパラメータの性質が異なります。\
リッジ回帰でパラメータ$\beta$が0に近づくにつれ、L2正則化項はより急速に0に近づく。一方Lasso回帰では、正則化項は$beta$の絶対値の和になるため、\
正則化項は$\beta$に比例して小さくなる。結果として、Lasso回帰では$\lambda$の値を大きくするにつれ、多くのパラメータが急速に0に近づくため、より重要なパラメータ以外の\
枝葉のパラメータが自動的に覗かれてくる。特に多くのパラメータを保持したくない場合（例えば携帯端末に保存するモデルを作るときなど）はこちらの方が適していると言える。\
以下のグラフはCreditデータセットに対してLasso回帰を行い、$\lambda$を大きくした際のパラメータと出力のCoefficientを示しており、$ \lambda$が大きく\
なるにつれて、多くのパラメータが急速に0に近づいていることがわかる。
\begin{figure}
  \begin{center}
    \includegraphics[width=13cm]{img/lasso_lmd.png}
    \caption{Creditデータセットに対してLasso回帰}
  \end{center}
\end{figure}

リッジ回帰、Lasso回帰どちらが適切なのかはデータの性質によって異なる。また、適切な$\lambda$の選択については、黄砂検証によってもっとも$Test MSE$が小さくなる$\lambda$を選択する。
%beta_iで変二分するとiの部分だけ残る
%らら
\subsection{交差検証による$\lambda$のチューニング}
リッジ回帰、Lasso回帰における正則化項にかかる係数$\lambda$の調整においては以前の項で紹介された交差検証により行います。様々な $\lambda$の値に対して交差検証でTest MSE を推定し、最も精度が良くなる$\lambda$を選びます。\
゙そのほか補足
Elastic Net->L1正則化項とL2正則化項の両方を追加 \\
最適化(パラメータ推定)->L1正則化では勾配の計算が難しくなる。L2は事情が並んでいるだけなので簡単。L1は絶対値があるので簡単ではない。最適化が難しくなってしまう。\
パラメータを減らすことができるという利点はあるが。データサイズが大きい時は勾配の計算（数値計算）が面倒になる。

\subsection{高次元データでの学習}
入力データによっては、予測変数（特徴量）$p$が学習事例の個数$n$よりも大きくなることが大きくなることがある。例えば、血圧の予測では、特徴量として年齢、性別、BMIの他に遺伝子情報（SNPs）を特徴量として入れる場合、
遺伝子情報は個々人によって少しずつ異なりそれぞれにダミー変数を振り分けていくと予測変数の数は膨大に増加する（$p \approx 500000$）。仮に患者データが200人分ほど($n \approx 200$)であれば$p \gg n$となってしまう。\
他にもスパムメールの分類で単語を特徴量として用いる時、すでに{スパム、スパムでない}のラベルがついたメールが1000件前後しかない時に、そこに含まれている全ての単語を特徴量として用いれば、\
特徴量の数はデータの数をたやすく上回ってし舞います。$p \geq n$のとき、必要ではないノイズな予測変数（例えばスパムメール内でも実際には分類に大きく関係しない単語は多々存在しています）でもフィットしてしまうため、\
過学習に陥る可能性が高くなります。

これに対する対処法としては、正則化や変数選択などによりモデルの柔軟性を下げ、必要ではない予測変数を減らします。しかしこの方法にはいくつか注意点があり、例えば多重共線性$\footnote{ある変数間で相関がある時、それが解析に支障が出るほどX同士の相関が強い時に、「多重共線性（たじゅうきょうせんせい）があると言います。\
http://xica.net/vno4ul5p/}$により、変数選択によって選ばれた変数以外にも有用な変数がぞんざいする可能性があります。\
また学習データで得られる{\rm RSS}や決定係数は、実際のデータでも完全に当てはまるとはいえず、モデル の性能の指標としては無意味ですことも注意が必要です。

\section{決定木(decision tree)}
木構造によってて回帰や分類のための条件を表すモデルであり、回帰木（regression tree）、分類木(classification tree)がある。\
\subsection{回帰木}
回帰木とは、ある数値（連続値）の推定のルールをツリーで表現したものです。
Hittersデータセット$\footnote{https://vincentarelbundock.github.io/Rdatasets/doc/vcd/Hitters.html}$は、選手の在籍年数（Years）と打数(Hits)よりLog(Salary)を予測するデータセットです。回帰木を用いてSalaryを予測する時、以下のようにまず「Years < 4.5」なら選手のLog(Salary)は5.11に決定され、\
そうでない、かつ「Hits < 117.5」ならば6.00、「Years < 4.5」かつ「Hits $\geq$ 117.5」ならば6.74となる。
\begin{figure}
  \begin{center}
    \includegraphics[width=10cm]{img/kaikiki.png}
    \caption{Hittersデータに対する回帰木予測}
  \end{center}
\end{figure}
回帰木は予測変数の空間を$K$個のリージョンに分割することにより、分岐条件を求める。

回帰木の大まかの作り方は以下のようになります。
\begin{enumerate}
  \item 予測変数の空間を$J$個の領域$R_1, R_2, \ldots , R_j$に区分けをする。
  \item 予測変数の値が領域$R_j$に含まれる入力に対し、$R_j$に含まれる学習データの応答変数の平均値を出力する。
\end{enumerate}
では予測変数の空間を$J$個の領域$R_1, R_2, \ldots , R_j$に区分ける方法として、${\rm RSS}$を最小化する区分けは計算量的に難しいため、とりあえず今ある空間を全ての予測変数と分割の基準値の組み合わせを考え、\
学習データの正解ラベルとモデルによって予測された値の差分の二乗和が最小になるよっっっっそく変数と分割の基準値を見つける方法があります。
\begin{itembox}[l]{貪欲法による領域の分割}
  全ての予測変数$X_1, X_2, \dots, X_p$と分割の基準値sを組み合わせ、以下が最小になる分割方法を選択する。\
  $$\sum_{i:x_i \in R_1(j, s)} (y_i - \hat{y_{R_i})^2 + R_2(j, s) (y_i - \hat{y_{R_2}})^2}$$
  $$ R_1(j, s) = {X|X_j < s}$$
  $$ R_2(j, s) = {X|X_j \geq s}$$
\end{itembox}
また、分割によって作られた二つの領域をさらに同様に分割し、領域に含まれる事例の数が閾値以下になるまで繰り返し再帰的に領域を構築していくが、\
こういった再帰的な分割方法だと領域同士が入り組んだ分割を行うことはできない。

回帰木はデータに対して容易にオーバーフィットしてしまうため、枝刈りによって木のサイズを小さくして過学習を抑制する必要があります。
\begin{itembox}[l]{枝刈り}
  一旦木を構築してしまってから、Cost Complexity Pruningによって多すぎる枝分かれ（領域）を減らしていくこと。\
  $$\sum_{m=1}^{|T|}\sum_{i:x_i \in R_m} (y_i - \hat{y}_{Rm})^2 + \alpha|T|$$
  ここでTは木のノード数です。
\end{itembox}
Cost Complexity Pruningは、木のノードの数（すなわち枝分かれの数）に対してペナルティをつけて行き、ただ{\rm RSS}でフィットさせるのではなく、どこの枝同士をマージすべきかを計算して求めます。ここで$\alpha$は任意の値であり、\
$\alpha$を増やすと木は縮みます。このチューニングも縮小推定の$\lambda$のチューニングと同様、黄砂検証によって行います。\
実際にHittersデータでは、枝仮によってある程度エラーが。。。

\subsection{分類木}
分類木とは質的変数（カテゴリなど）を応答変数（予測したい値、$Y$）とし、末端ノードの出力は回帰木のような平均ではなく、多数決でラベルを決定する。\
分類木の構築は回帰木と同様、再帰的な分割を繰り返すことにより行います。しかしここで値を求める際の指標では分類誤り率（正しくカテゴリが予測されているか否か）ではなく、\
Gini indexやentropy等が用いられることが多いです。単なる正解率で考えないのは多数決により分類木のラベルを決定するため、単純な正解率によるモデル評価はこの多数決で少数派となったラベルについての情報が考慮されなくなってしまうためです。

\begin{itembox}[l]{Gini indeとentropy}
  $\hat{p}_{mk}$を領域mに含まれるクラスkの事例の割合とすると、
  \begin{description}
    \item [Gini index]\mbox{}\\
    $$G = \sum_{k=1}^K\hat{p}_{mk}(1 - \hat{p}_{mk})$$
    \item [entropy]\mbox{}\\
    $$D = -\sum_{k=1}^K \hat{p}_{mk}\log{\hat{p}_{mk}}$$
  \end{description}
\end{itembox}

% \begin{figure}
%   \begin{center}
%     \includegraphics[width=8cm]{img/class_tree.png}
%     \caption{分類木予測}
%   \end{center}
% \end{figure}
枝刈りの結果、NOとNO、YESとYESに分岐するノードが存在するが、これは予測するクラスは同じではあるが、「どのくらいの確率でYES/NOか」が異なってくる。

\begin{figure}[htbp]
\begin{minipage}{0.7\hsize}
 \begin{center}
  \includegraphics[width=100mm]{img/class_orig.png}
 \end{center}
 \caption{分類木によるHeartデータセット予測（枝刈り後)}
 \label{fig:one}
\end{minipage}
\begin{minipage}{0.3\hsize}
 \begin{center}
  \includegraphics[width=50mm]{img/class_tree.png}
 \end{center}
 \caption{分類木によるHeartデータセット予測（枝刈り後)}
 \label{fig:two}
\end{minipage}
\end{figure}

\subsection{決定木の長所短所}
決定木の長所としては、以下の二点を上げることができます。
\begin{itemize}
  \item モデルが実際に利用した条件分岐などを確認でき、またそれも極めて人間の行う意思決定の方法に近いため学習結果の解釈が容易。
  \item 質的変数を扱う際にダミー変数を作らなくてよい
\end{itemize}
逆に短所としては
\begin{itemize}
  \item 回帰や分類の精度はあまり高くない（特殊なデータを除き、一般的な回帰モデル、分類モデルの方が高い精度を示すことが多い）
  \item 学習データにオーバーフィットしやすいため、学習データの小さな違いで構築される木が大きく変わることがある
\end{itemize}
例えば映画レビューをネガティブなものかポジティブなものか判断するとき、決定木モデルを用いるとある単語の有無で条件分岐を行うため、\
とてもネガティブな内容ではあるものの、学習データのポジティブなメールに多く含まれていた単語が多用されているレビューに対しては正しく予測できない可能性がより高くなる。\
ただ決定木はアンサンブル学習の「ランファムフォレスト」、ブースティング等により制度の向上が可能で、以下では以下に決定機による予測精度を向上するかについて説明します。

\subsection{バギング}
バギングはブートルトラップ（一つ抜き法によるサンプリング）によってバリアンス（学習データからのずれ）を減らすことを目指す方法です。\
この出力は回帰木の出力の平均になります。Out-of-bag (OOB) エラー推定が便利です。普通は交差検定などを行う必要があるが、

\section{Rチートシート}

\section{参考資料}
授業で「参考」とされた内容や数式の計算過程等をまとめてあります。
\subsection{リッジ回帰、Lasso回帰を不等式制約付きの最適化問題として捉える}
リッジ回帰、Lasso回帰は以下のような不等式条件付きの最適化問題として捉えることができ、この見方をするとなぜLasso回帰においてパラメータが0に収束しやすいのか\
理解がしやすくなります。

\begin{itembox}[l]{不等式条件つき最適化問題としての等式化}
  \begin{description}
    \item[リッジ回帰]\mbox{}\\
    \begin{equation*}
      \begin{aligned}
      & \underset{\beta}{\text{minimize}}
      & &\displaystyle{\sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p \beta_jx_{ij}\right)^2 }  \\
      & \text{subject to}
      & & \lambda \sum_{j=1}^p \beta_j^2 \leq s
      \end{aligned}
    \end{equation*}
    \item[Lasso回帰]\mbox{}\\
      \begin{equation*}
        \begin{aligned}
        & \underset{\beta}{\text{minimize}}
        & &\displaystyle{\sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p \beta_jx_{ij}\right)^2 }  \\
        & \text{subject to}
        & & \lambda \sum_{j=1}^p |\beta_j| \leq s
        \end{aligned}
      \end{equation*}
  \end{description}
  %
  % 縮小推定別な表現の方法も
  % ゼロでない変数が最大s個という制約を加えることも可能。これはまずい〜ー＞勾配を見ながらbetaを探しているw毛ではない、
  % 製薬の形で考えると、なぜらっそだとパラメータがゼロになるかがわかる
  % らっそだとダイヤモンドの中に入る必要。雪片あたりで{\rm RSS}の等高線と一致する
  % リッジ回帰は事情ノルムの制約がつく。ベータ１も2もゼロになることはない
  % Lasso v.s. リッジ
  % 問題によって変わります
\end{itembox}
\subsection{予測誤差の大きさ（第3講）}
予測結果$\hat{Y}$と実際のラベル$Y$の二乗誤差の期待値を計算してみる。
統計的機械学習において、$Y = f(X) + \epsilon$で表されるため、
$$E[(Y - \hat{Y})^2] = E[((f(X) + \epsilon) - \hat{f}(X))^2]$$

\subsection{Bias-variance trade-offの導出（第4講）}



\subsection{線形単回帰のパラメータの推定法(第4講)}
線形単回帰の{\rm RSS}を最小にする$\hat{\beta}_0, \hat{\beta}_1$は、ぞれぞれで{\rm RSS}を偏微分し、偏微分した結果を0にする$\hat{\beta}_0, \hat{\beta}_1$を\
連立方程式にとけば求めることができます。
$${\rm {\rm RSS}} = (y_1 - \hat{\beta_0} - \hat{\beta_1}x_1)^2 +  (y_2 - \hat{\beta_0} - \hat{\beta_1}x_2)^2 \ldots  (y_n - \hat{\beta_0} - \hat{\beta_1}x_n)^2$$
% (a + b + c)^2 = a^2 + b^2 + c^2 + 2ab + 2bc + 2ca
$$(y_1 - \hat{\beta_0} - \hat{\beta_1}x_1)^2 = y_1^2 + \hat{\beta_0}^2 + \hat{\beta_1}^2x_1^2 - 2y_1\hat{\beta_0} + 2\hat{\beta_0}\hat{\beta_1}x_1 -2\hat{\beta_1}x_1y_1より$$
% $$\frac{\partial {\rm RSS}}{\partial \hat{\beta_0}} = 0より$$
% $$\sum_{i=1}^n y_i - \beta_1\sum_{i=1}^n x_i - \beta_0 n = 0$$
% $$\frac{\partial {\rm RSS}}{\partial \hat{\beta_1}} = 0より$$
\begin{numcases}
  {}
  \frac{\partial {\rm RSS}}{\partial \hat{\beta_0}} = 0 \iff \sum_{i=1}^n y_i - \beta_1\sum_{i=1}^n x_i - \beta_0 n = 0  & \\
  \frac{\partial {\rm RSS}}{\partial \hat{\beta_1}} = 0 \iff \sum_{i=1}^n x_iy_i - \beta_1\sum_{i=1}^n x_i^2 - \beta_0 \sum_{i=1}^n x_i = 0  &
\end{numcases}
式(3)より
\begin{equation}
  \beta_0 = \frac{1}{n}\sum_{i=1}^n y_i - \beta_1\sum_{i=1^n}x_i
\end{equation}
ここで
\begin{equation}
  \overline{x} = \frac{1}{n}\sum_{i=1}^n x_i, \overline{y} = \frac{1}{n}\sum_{i=1}^n y_i
\end{equation}
式(5)(6)を式(4)に代入し、$\beta_1$について解くと、
\begin{equation*}
  \begin{split}
    \beta_1 &=  \frac{\sum_{i=1}^n x_iy_i - \frac{1}{n}(\sum_{i=1}^n y_i\sum_{i=1}^n x_i)}{\sum_{i=1}^n x_i^2 - \frac{\sum_{i=1}^n x_i^2}{n}} \\
    &= \frac{\sum_{i=1}^n(x - \overline{x})(y - \overline{y})}{\sum_{i=1}^n (x - \overline{x})^2}
  \end{split}
\end{equation*}
また式(5)より
$$\beta_0 = \overline{y} - \beta_1\overline{x}$$
\subsection{{\rm RSS}の性質の導出(第4講)}
式(5)について、全て一つのシグマでくくると
\begin{equation}
  \sum_{i=1}^n y_i - \beta_1x_i - \beta_0 = \sum_{i=1}^n e_i = 0
\end{equation}
よって、{\rm RSS}に関する一つ目の性質が示された。
また同様に式(6)より
\begin{equation}
  \sum_{i=1}^n x_iy_i - \beta_1x_i^2 - \beta_0 x_i = \sum_{i=1}^n x_i (y_i - \beta_1x_i - \beta_0)  =  \sum_{i=1}^n e_ix_i = 0
\end{equation}
よって二つ目の性質も{\rm RSS}からパラメータを推定する際に用いた式より導くことができる。

\subsection{標準誤差の求め方及び標準偏差(第4)}
母平均の信頼区間$ = 標本平均 \pm t \times 標本標準偏差 \div \sqrt{標本数}$です。
$$標準誤差 SE = \frac{標準偏差 SD}{\sqrt{標本数 N}}$$より、パラメータ$\beta_0, \beta_1$の信頼区間はそれぞれ
$$\hat{\beta}_0 \pm t{\rm SE}(\hat{\beta}_0), \hat{\beta}_1 \pm t{\rm SE}(\hat{\beta}_1)$$となる。
tの値はt分布表からtの値を求めることができる。信頼区間の当たる確率、すなわち95$\%$信頼区間であれば95$\%$を信頼係数といい、\
標本数から1を引いた数を自由度と呼び、t分布表ではこの自由度と信頼係数よりtの値を求めることができます。\
例えば標本数が10で95$\%$信頼区間を求めたい時、以下の表よりtは自由度が9、信頼係数が95$\%$の2.262ですことがわかる。\
標本数が大きくなるとこのtの値はほぼ正規分布の値と変わらず2前後になるため、線形単回帰のパラメータの信頼区間の推定において近似的に$t=2$とすることができる。
（ザックりな説明ですみません… https://blog.apar.jp/data-analysis/4632/)
\begin{figure}
  \begin{center}
  \includegraphics[width=10cm]{img/t_bunpu.png}
  \caption{t分布表の見方}
\end{center}
\end{figure}

\subsection{縮小推定の手計算（第10講）}
$n = p, \beta_0 = 0$
\[
  X = \left(
    \begin{array}{cccc}
      1 & 0 & \ldots & 0 \\
      0 & 1 & \ldots & 0 \\
      \vdots & \vdots & \ddots & \vdots \\
      0 & 0 & \ldots & 1
    \end{array}
  \right)
\]
という問題に対して、パラメータ推定及び縮小推定（リッジ推定、Lasso推定）を行う。

まず、最小二乗法でパラメータを推定する際には、
最小二乗法を用いる場合、その目的関数は以下のようになる。目的関数を最小化する$\beta_i$は目的関数の微分が0になる時となるため、
\begin{equation}
  L = \sum_{j = 1}^p(y_i - \beta_j)^2
\end{equation}
$$\frac{\partial L}{\partial \beta_i} = \frac{\partial}{\partial \beta_i}\sum_{j = 1}^p(y_i - \beta_j)^2 = \frac{\partial}{\partial \beta_i}(y_i^2 - 2y_i\beta_i + \beta_i^2) = -2(y_i - \beta_i)$$
$L$に対して$\beta_i$の微分を取る時、$i$番目以外のパラメータ$\beta$は無視してよいので、結果的に$L$を最小にする$\beta_i$の値は以下のようになる。

\begin{equation}
  \beta_i = y_i
\end{equation}

リッジ回帰においては目的関数が以下(5)で表現されるため、同様に$\beta_i$で微分をし、Lを最小化する$\beta_i$を求めると以下の(6)のようになる。
\begin{equation}
L = \sum_{i=1}^n \left(y_i - \beta_i \right)^2 + \lambda \sum_{j=1}^p \beta_j^2
\end{equation}

$$\frac{\partial L}{\partial \beta_i} = \frac{\partial}{\partial \beta_i}\left(\sum_{j = 1}^p(y_i - \beta_j)^2 +\lambda \sum_{j=1}^p \beta_j^2\right) + 2\lambda\beta_i = \frac{\partial}{\partial \beta_i}(y_i^2 - 2y_i\beta_i + \beta_i^2) = 2(1 + \beta_i) - 2y_i $$
\begin{equation}
  \hat{\beta_i^R} = \frac{y_i}{1 + \lambda}
\end{equation}
(6)より、$\lambda = 0$ならば目的関数を最小化する$\beta_i$がパラメータ推定の結果求められた(5)と一致することがわかり、\
$\lambda = 0$の縮小推定の結果はパラメータ推定により求められる値と一致することが確認できる。

Lasso回帰においては目的関数が以下(7)で表現される。Lasso回帰においては、正則化項が絶対値符号を含むため、$\beta_i$が正か負かで場合わけをする必要がある。
\begin{equation}
  L = \sum_{i=1}^n \left(y_i - \beta_i \right)^2 + \lambda \sum_{j=1}^p |\beta_j|
\end{equation}

% \begin{equation}
%   \frac{\partial L}{\partial \beta_i} = \frac{\partial}{\partial \beta_i}\left(\sum_{j = 1}^p(y_i - \beta_j)^2 +\lambda \sum_{j=1}^p \beta_j^2\right) + 2\lambda\beta_i = \frac{\partial}{\partial \beta_i}(y_i^2 - 2y_i\beta_i + \beta_i^2) = 2(1 + \beta_i) - 2y_i
%
% \end{equation}
















\end{document}
